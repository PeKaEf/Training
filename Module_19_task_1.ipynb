{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2eac9f-41b0-413e-a00d-c05578e093c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "!wget --no-check-certificate \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\" -O \"/tmp/cats-and-dogs.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e2f0f5-08d0-47fb-ac85-a0ee7128cb5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/cats-and-dogs.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct_files\n\u001b[0;32m     10\u001b[0m local_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/cats-and-dogs.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m zip_ref \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_zip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m zip_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m zip_ref\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\zipfile.py:1281\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1281\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/cats-and-dogs.zip'"
     ]
    }
   ],
   "source": [
    "def get_valid(file_path):\n",
    "    correct_files = []\n",
    "    for name in os.listdir(file_path):\n",
    "        try:\n",
    "            img = Image.open(file_path + \"/\" + name)\n",
    "            correct_files.append(name)\n",
    "        except UnidentifiedImageError:\n",
    "            pass\n",
    "    return correct_files\n",
    "local_zip = 'tmp/cats-and-dogs.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "classes = ['Cat', 'Dog']\n",
    "original_cat_path = '/tmp/PetImages/Cat'\n",
    "original_dog_path = '/tmp/PetImages/Dog'\n",
    "original_cat = get_valid(original_cat_path)\n",
    "original_dog = get_valid(original_dog_path)\n",
    "random.seed(101)\n",
    "\n",
    "random.shuffle(original_cat)\n",
    "random.shuffle(original_dog)\n",
    "size = min(len(original_cat), len(original_dog))\n",
    "train_size = int(np.floor(0.7 * size))\n",
    "\n",
    "valid_size = int(np.floor(0.2 * size))\n",
    "test_size = size - train_size - valid_size\n",
    "base_directory = 'dataset'\n",
    "os.mkdir(base_directory)\n",
    "type_datasets = ['train', 'valid', 'test']\n",
    "directories = {}\n",
    "\n",
    "for type_dataset in type_datasets:\n",
    "    directory = os.path.join(base_directory, type_dataset)\n",
    "    os.mkdir(directory)\n",
    "    for name_class in classes:\n",
    "        animal = os.path.join(directory, name_class)\n",
    "        os.mkdir(animal)\n",
    "        directories[f'{type_dataset}_{name_class}'] = animal+'/'\n",
    "index = 0\n",
    "\n",
    "for name_cat, name_dog in zip(original_cat, original_dog):\n",
    "        if index <= train_size:\n",
    "            type_of_dataset = 'train'\n",
    "        elif train_size < index <= (train_size + valid_size):\n",
    "            type_of_dataset = 'valid'\n",
    "        elif (train_size + valid_size) < index <= (train_size + valid_size + test_size):\n",
    "            type_of_dataset = 'test'\n",
    "        shutil.copyfile(src=(original_cat_path + '/' +name_cat), dst=(directories[f'{type_of_dataset}_Cat']+name_cat))\n",
    "        shutil.copyfile(src=(original_dog_path + '/' + name_dog), dst=(directories[f'{type_of_dataset}_Dog']+name_dog))\n",
    "        index += 1\n",
    "\n",
    "print(f'Dog - train: {len(os.listdir(directories[\"train_Dog\"]))}\\tCat - train: {len(os.listdir(directories[\"train_Cat\"]))}')\n",
    "print(f'Dog - valid: {len(os.listdir(directories[\"valid_Dog\"]))}\\tCat - valid: {len(os.listdir(directories[\"valid_Cat\"]))}')\n",
    "print(f'Dog - test:  {len(os.listdir(directories[\"test_Dog\"]))}\\tCat - test:  {len(os.listdir(directories[\"test_Cat\"]))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35d9d5c-0537-4498-8ea7-1851bdddffe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'directories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m      2\u001b[0m fig\u001b[38;5;241m.\u001b[39msubplots_adjust(hspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, wspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdirectories\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_Cat\u001b[39m\u001b[38;5;124m\"\u001b[39m]))[:\u001b[38;5;241m8\u001b[39m]):\n\u001b[0;32m      4\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(directories[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_Cat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39melement)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'directories' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (12, 12))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for i, element in enumerate(os.listdir(os.path.join(directories[\"train_Cat\"]))[:8]):\n",
    "    ax = fig.add_subplot(4, 4, i+1)\n",
    "    img = Image.open(directories[\"train_Cat\"]+element)\n",
    "    ax.imshow(img)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i, element in enumerate(os.listdir(os.path.join(directories[\"train_Dog\"]))[:8]):\n",
    "    ax = fig.add_subplot(4, 4, i+9)\n",
    "    img = Image.open(directories[\"train_Dog\"]+element)\n",
    "    ax.imshow(img)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241f6080-e1f8-47c2-9e61-83d98a350a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m----> 6\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_size\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m      7\u001b[0m validation_steps \u001b[38;5;241m=\u001b[39m valid_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m      8\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_size' is not defined"
     ]
    }
   ],
   "source": [
    "img_width, img_height = 150, 150\n",
    "train_data_dir = 'dataset/train/'\n",
    "validation_data_dir = 'dataset/valid/'\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "steps_per_epoch = train_size // batch_size\n",
    "validation_steps = valid_size // batch_size\n",
    "patience = 5\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size, class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size, class_mode='binary')\n",
    "train_datagen_augmentation = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   rotation_range=30,\n",
    "                                   horizontal_flip=True)\n",
    "train_generator_augmentation = train_datagen_augmentation.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size, class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacff81-d685-43af-9860-2b9e65972a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = Sequential()\n",
    "model_baseline.add(Flatten(input_shape=(150, 150, 3)))\n",
    "model_baseline.add(Dense(units=1, activation='sigmoid'))\n",
    "model_baseline.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(learning_rate=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_baseline.summary()\n",
    "models.append(\"baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f011a-aebf-406e-a352-a8838364d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=patience, monitor='val_accuracy', restore_best_weights=True)\n",
    "history_baseline = model_baseline.fit_generator(train_generator,\n",
    "                                                steps_per_epoch=steps_per_epoch,\n",
    "                                                epochs=epochs,\n",
    "                                                validation_data=validation_generator,\n",
    "                                                validation_steps=validation_steps,\n",
    "                                                callbacks=[es])\n",
    "history_baseline_df = pd.DataFrame(history_baseline.history)\n",
    "history_baseline_csv_file = 'history/history_baseline.csv'\n",
    "\n",
    "with open(history_baseline_csv_file, mode='w') as f:\n",
    "    history_baseline_df.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d2200-f5b2-46fe-8771-709c4a6e0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = 0\n",
    "min_accuracy = 1\n",
    "max_loss = 0\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(models)))\n",
    "\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    if max_index < max(df.index):\n",
    "        max_index = max(df.index)\n",
    "    if min_accuracy > min(df[['accuracy', 'val_accuracy']].min()):\n",
    "        min_accuracy = min(df[['accuracy', 'val_accuracy']].min())\n",
    "    if max_loss < max(df[['loss', 'val_loss']].max()):\n",
    "        max_loss = max(df[['loss', 'val_loss']].max())\n",
    "\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.plot(df['accuracy'], \"bp--\")\n",
    "    ax.plot(df['val_accuracy'], \"rp--\")\n",
    "    ax.set_title(f'Model {model} Accuracy', fontsize=20)\n",
    "    ax.set_ylabel('Accuracy', fontsize=15)\n",
    "    ax.set_xlabel('Epoch', fontsize=15)\n",
    "    ax.set_xlim([1, max_index])\n",
    "    ax.set_ylim([min_accuracy, 1])\n",
    "\n",
    "    for milestone in (0.7, 0.8, 0.9, 0.95):\n",
    "        ax.axhline(milestone, color=\"k\", linestyle=\"--\")\n",
    "        try:\n",
    "            if min(df[df['val_accuracy'] >= milestone].index) > 1:\n",
    "                plt.axvline(min(df[df['val_accuracy'] >= milestone].index), color=\"g\", linestyle=\"--\")\n",
    "                ax.text(min(df[df['val_accuracy'] >= milestone].index)+0.6, min_accuracy+0.02,\n",
    "                        f'Epoch: {min(df[df[\"val_accuracy\"] >= milestone].index)}', rotation=90)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    ax = fig.add_subplot(212)\n",
    "    ax.plot(df['loss'], \"bp--\")\n",
    "    ax.plot(df['val_loss'], \"rp--\")\n",
    "    ax.set_title(f'Model {model} Loss', fontsize=20)\n",
    "    ax.set_ylabel('Loss', fontsize=15)\n",
    "    ax.set_xlabel('Epoch', fontsize=15)\n",
    "    ax.set_xlim([1, max_index])\n",
    "    ax.set_ylim([0, max_loss])\n",
    "    ax.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'charts/train_history_{model}.png', transparent=True, dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_subplot(211)\n",
    "for model, color in zip(models, colors):\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    ax.plot(df['val_accuracy'], label=f'Model {model}', color=color, linewidth=3)\n",
    "    ax.axhline(df['val_accuracy'].max(), color=color, linestyle=\"dotted\", linewidth=4)\n",
    "\n",
    "ax.set_title(f'Accuracy', fontsize=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=15)\n",
    "ax.set_xlabel('Epoch', fontsize=15)\n",
    "ax.set_xlim([1, max_index])\n",
    "ax.set_ylim([min_accuracy, 1])\n",
    "for milestone in (0.7, 0.8, 0.9, 0.95):\n",
    "    ax.axhline(milestone, color=\"k\", linestyle=\"--\")\n",
    "plt.legend(loc='lower right')\n",
    "ax = fig.add_subplot(212)\n",
    "for model, color in zip(models, colors):\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    ax.plot(df['val_loss'], label=f'Model {model}', color=color, linewidth=3)\n",
    "    ax.axhline(df['val_loss'].min(), color=color, linestyle=\"dotted\", linewidth=4)\n",
    "ax.set_title(f'Loss', fontsize=20)\n",
    "ax.set_ylabel('Loss', fontsize=15)\n",
    "ax.set_xlabel('Epoch', fontsize=15)\n",
    "ax.set_xlim([1, max_index])\n",
    "ax.set_ylim([0, max_loss])\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'charts/train_history_of_each_model.png', transparent=True, dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575945a3-c366-4b3b-9533-91672c5179a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_1 = Sequential()\n",
    "model_simple_1.add(Conv2D(filters=10, kernel_size=(3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model_simple_1.add(MaxPooling2D(2, 2))\n",
    "model_simple_1.add(Conv2D(2, 2),activation='relu')\n",
    "model_simple_1.add(Flatten())\n",
    "model_simple_1.add(Dense(units=1, activation='sigmoid'))\n",
    "model_simple_1.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "model_simple_1.summary()\n",
    "models.append(\"simple_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00a67f-eb32-4147-ab5c-e1fb03ba2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_1 = model_simple_1.fit_generator(train_generator,\n",
    "                                                steps_per_epoch=steps_per_epoch,\n",
    "                                                epochs=epochs,\n",
    "                                                validation_data=validation_generator,\n",
    "                                                validation_steps=validation_steps,\n",
    "                                                callbacks=[es])\n",
    "\n",
    "history_simple_1_df = pd.DataFrame(history_simple_1.history)\n",
    "history_simple_1_csv_file = 'history/history_simple_1.csv'\n",
    "\n",
    "with open(history_simple_1_csv_file, mode='w') as f:\n",
    "    history_simple_1_df.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ef4de-a0d8-4e3c-bb18-93004bbeef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_2 = Sequential()\n",
    "model_simple_2.add(Conv2D(filters=10, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(150, 150, 3)))\n",
    "model_simple_2.add(MaxPooling2D(2, 2))\n",
    "model_simple_2.add(Flatten())\n",
    "model_simple_2.add(Dense(units=1, activation='sigmoid'))\n",
    "model_simple_2.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "model_simple_2.summary()\n",
    "models.append(\"simple_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7ff5d-8890-407c-acca-9a670c53871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model_1.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(150,150,3)))\n",
    "model_1.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 2\n",
    "model_1.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 3\n",
    "model_1.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 4\n",
    "model_1.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(256, activation='relu'))\n",
    "model_1.add(Dense(128, activation='relu'))\n",
    "model_1.add(Dense(units=1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_1.summary()\n",
    "models.append(\"model_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e1490-1245-4e97-8129-6be7e20ad130",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model_2.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(150,150,3)))\n",
    "model_2.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 2\n",
    "model_2.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 3\n",
    "model_2.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 4\n",
    "model_2.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(256, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(128, activation='relu'))\n",
    "model_2.add(Dense(units=1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_2.summary()\n",
    "models.append(\"model_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7ca7e-2e6e-422d-a525-2e149df99d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model_3.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(150,150,3)))\n",
    "model_3.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 2\n",
    "model_3.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 3\n",
    "model_3.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 4\n",
    "model_3.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_3.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(256, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(128, activation='relu'))\n",
    "model_3.add(Dense(units=1, activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_3.summary()\n",
    "models.append(\"model_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf694b-2b22-410b-b959-a712f83f6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model_3 = model_3.fit_generator(train_generator_augmentation,\n",
    "                                        steps_per_epoch=steps_per_epoch,\n",
    "                                        epochs=epochs,\n",
    "                                        validation_data=validation_generator,\n",
    "                                        validation_steps=validation_steps,\n",
    "                                        callbacks=[es])\n",
    "\n",
    "history_model_3_df = pd.DataFrame(history_model_3.history)\n",
    "history_model_3_csv_file = 'history/history_model_3.csv'\n",
    "with open(history_model_3_csv_file, mode='w') as f:\n",
    "    history_model_3_df.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4604e-f14f-45d1-a42c-b0d58831b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "vgg16.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in vgg16.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "for layer in vgg16.layers:\n",
    "    print(f'layer_name: {layer.name:13} trainable: {layer.trainable}')\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(vgg16)\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(256, activation='relu'))\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(128, activation='relu'))\n",
    "model_4.add(Dense(units=1, activation='sigmoid'))\n",
    "model_4.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_4.summary()\n",
    "models.append(\"model_4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
